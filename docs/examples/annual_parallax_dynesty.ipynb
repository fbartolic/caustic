{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "%run notebook_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Â Nested sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we fit the same event with the same model as in the annual parallax example, except using dynamic nested sampling as implemented in [dynesty](https://dynesty.readthedocs.io/en/latest/index.html) instead of Hamiltonian Monte Carlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import pymc3 as pm\n",
    "import theano.tensor as T\n",
    "\n",
    "import caustic as ca\n",
    "import exoplanet as xo\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load event data\n",
    "event_dir = \"../../data/OB05086/\"\n",
    "event = ca.data.OGLEData(event_dir)\n",
    "\n",
    "# Plot data\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "event.plot_standardized_data(ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a SingleLensModel object\n",
    "parallax_model = ca.models.SingleLensModel(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bands = len(event.light_curves)\n",
    "BoundedNormal = pm.Bound(pm.Normal, lower=0.0)\n",
    "BoundedNormal_1 = pm.Bound(pm.Normal, lower=1.0)\n",
    "\n",
    "with parallax_model:\n",
    "    # Flux parameters\n",
    "    Delta_F = BoundedNormal(\n",
    "        \"Delta_F\",\n",
    "        mu=T.zeros(n_bands),\n",
    "        sd=50.0 * T.ones(n_bands),\n",
    "        testval=5.0 * T.ones(n_bands),\n",
    "        shape=(n_bands),\n",
    "    )\n",
    "\n",
    "    F_base = pm.Normal(\n",
    "        \"F_base\",\n",
    "        mu=T.zeros(n_bands),\n",
    "        sd=0.6 * T.ones(n_bands),\n",
    "        testval=T.zeros(n_bands),\n",
    "        shape=(n_bands),\n",
    "    )\n",
    "\n",
    "    # Other parameters\n",
    "    t_0 = pm.Uniform(\n",
    "        \"t_0\", parallax_model.t_min, parallax_model.t_max, testval=ca.estimate_t0(event)\n",
    "    )\n",
    "\n",
    "    # In the parallax model, u_0 can be negative\n",
    "    u_0 = pm.Normal(\"u_0\", mu=0.0, sd=1.5, testval=-0.41)\n",
    "    teff = BoundedNormal(\"t_eff\", mu=0.0, sd=365.0, testval=20.0)\n",
    "\n",
    "    # Initialize the two parallax parameters\n",
    "    pi_EE = pm.Normal(\"pi_EE\", mu=0.0, sigma=1.0, testval=0.1)\n",
    "    pi_EN = pm.Normal(\"pi_EN\", mu=0.0, sigma=1.0, testval=-0.3)\n",
    "\n",
    "    # Deterministic transformations\n",
    "    t_E = pm.Deterministic(\"t_E\", teff / T.abs_(u_0))\n",
    "    m_source, g = ca.compute_source_mag_and_blend_fraction(\n",
    "        event, parallax_model, Delta_F, F_base, u_0\n",
    "    )\n",
    "    pm.Deterministic(\"m_source\", m_source)\n",
    "    pm.Deterministic(\"g\", g)\n",
    "    pm.Deterministic(\"pi_E\", T.sqrt(pi_EE ** 2 + pi_EN ** 2))\n",
    "\n",
    "    # Compute the trajectory including parallax\n",
    "    trajectory = ca.trajectory.Trajectory(event, t_0, u_0, t_E, pi_EE, pi_EN)\n",
    "    u = trajectory.compute_trajectory(parallax_model.t)\n",
    "\n",
    "    # Compute the magnification\n",
    "    mag = parallax_model.compute_magnification(u, u_0)\n",
    "\n",
    "    # Compute the mean model\n",
    "    mean = Delta_F * mag + F_base\n",
    "\n",
    "    # Let's allow for rescaling of the error bars by a constant factor\n",
    "    c_1 = BoundedNormal_1(\n",
    "        \"c_1\",\n",
    "        mu=T.ones(n_bands),\n",
    "        sd=2.0 * T.ones(n_bands),\n",
    "        testval=1.5 * T.ones(n_bands),\n",
    "        shape=(n_bands),\n",
    "    )\n",
    "\n",
    "    # Diagonal terms of the covariance matrix\n",
    "    var_F = (c_1 * parallax_model.sig_F) ** 2\n",
    "\n",
    "    # Compute the Gaussian log_likelihood, add it as a potential term to the model\n",
    "    ll = parallax_model.compute_log_likelihood(parallax_model.F - mean, var_F)\n",
    "    pm.Potential(\"log_likelihood\", ll)\n",
    "\n",
    "    pm.Deterministic(\"log_likelihood_\", ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(parallax_model.vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use dynamic nested sampling, we have to re write the priors in the form of a prior transfer function which maps i.i.d uniformly distributed parameters defined on a unit cube to our parameters of interest. The `ppf` function associated with probability distributions defined in `scipy.stats` does exactly that. To learn why this step is necessary, check out the [dynesty docs](https://dynesty.readthedocs.io/en/latest/quickstart.html#prior-transforms). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "\n",
    "def prior_transform(u):\n",
    "    \"\"\"\n",
    "    Transforms the uniform random variables `u ~ Unif[0., 1.)`\n",
    "    to the parameters of interest.\n",
    "    \"\"\"\n",
    "    x = np.array(u)  # copy u\n",
    "\n",
    "    # Delta_F_lowerbound__\n",
    "    x[0] = scipy.stats.norm.ppf(u[0], loc=2.0, scale=1.0)\n",
    "\n",
    "    # F_base__\n",
    "    x[1] = scipy.stats.norm.ppf(u[1], loc=0.0, scale=0.5)\n",
    "\n",
    "    # t_0_interval__\n",
    "    x[2] = scipy.stats.norm.ppf(u[2], loc=0.1, scale=0.5)\n",
    "\n",
    "    # u_0\n",
    "    x[3] = scipy.stats.norm.ppf(u[3], loc=0.0, scale=1.0)\n",
    "\n",
    "    # t_eff_lowerbound__\n",
    "    x[4] = scipy.stats.norm.ppf(u[4], loc=3.0, scale=2.0)\n",
    "\n",
    "    # pi_EE\n",
    "    x[5] = scipy.stats.norm.ppf(u[5], loc=0.0, scale=0.5)\n",
    "\n",
    "    # pi_EN\n",
    "    x[6] = scipy.stats.norm.ppf(u[6], loc=0.0, scale=0.5)\n",
    "\n",
    "    # c_1_lowerbound\n",
    "    x[7] = scipy.stats.norm.ppf(u[7], loc=-3, scale=3.0)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the sampling, this will take some time..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will take a long time\n",
    "loglike = ca.utils.get_log_likelihood_function(\n",
    "    parallax_model, parallax_model.log_likelihood\n",
    ")\n",
    "sampler = dynesty.DynamicNestedSampler(\n",
    "    loglike, prior_transform, ndim, **{\"sample\": \"rwalk\"}\n",
    ")\n",
    "\n",
    "\n",
    "sampler.run_nested(\n",
    "    wt_kwargs={\"pfrac\": 1.0}, print_progress=True, **{\"nlive_init\": 1000}\n",
    ")\n",
    "\n",
    "results = sampler.results\n",
    "\n",
    "# Resample samples such that they have equal weight\n",
    "samples, weights = results.samples, np.exp(results.logwt - results.logz[-1])\n",
    "new_samples = dyfunc.resample_equal(samples, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sampling appears to have converged according to internal criteria specified in dynesty with a total of ~20M likelihood calls, let's plot the diagnostics plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dynesty import plotting as dyplot\n",
    "\n",
    "# Plot a summary of the run.\n",
    "# rfig, raxes = dyplot.runplot(results)\n",
    "\n",
    "# Plot traces and 1-D marginalized posteriors.\n",
    "tfig, taxes = dyplot.traceplot(results)\n",
    "\n",
    "# Plot the 2-D marginalized posteriors.\n",
    "cfig, caxes = dyplot.cornerplot(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the posterior over the $u_0$ parameter, which we expect to be multi-modal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot posterior for u_0\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.hist(samples[:, 3], bins=200, density=True, alpha=0.5)\n",
    "ax.set_xlim(-0.6, 0.6)\n",
    "ax.grid()\n",
    "ax.set_xlabel(\"$u_0$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like `dynesty` not only managed to discover both significant modes in the posterior, but also the relative height between the two modes matches what we've oberved in the [annual parallax example](annual_parallax.ipynb).\n",
    "\n",
    "Let's plot the model to check that it makes sense, and plot the posterior trajectories from the multi-modal pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with parallax_model:\n",
    "    # Create dense grid\n",
    "    t_dense = np.tile(\n",
    "        np.linspace(parallax_model.t_min, parallax_model.t_max, 1000), (n_bands, 1)\n",
    "    )\n",
    "    t_dense_tensor = T.as_tensor_variable(t_dense)\n",
    "\n",
    "    # Evaluate trajectory components on dense grid\n",
    "    u_n, u_e = trajectory.compute_trajectory(t_dense_tensor, return_components=True)\n",
    "\n",
    "    # Compute the magnification\n",
    "    mag_dense = parallax_model.compute_magnification(T.sqrt(u_n ** 2 + u_e ** 2), u_0)\n",
    "\n",
    "    # Compute the mean model\n",
    "    mean_dense = Delta_F * mag_dense + F_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot model\n",
    "fig, ax = plt.subplots(\n",
    "    2, 1, gridspec_kw={\"height_ratios\": [3, 1]}, figsize=(10, 8), sharex=True\n",
    ")\n",
    "\n",
    "ca.plot_model_and_residuals(\n",
    "    ax, event, parallax_model, samples, t_dense_tensor, mean_dense, n_samples=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot trajectory\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "ca.plot_trajectory_from_samples(\n",
    "    ax,\n",
    "    event,\n",
    "    parallax_model,\n",
    "    samples,\n",
    "    t_dense_tensor,\n",
    "    u_n,\n",
    "    u_e,\n",
    "    n_samples=100,\n",
    "    color=\"C0\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good. Whereas in the parallax example with HMC we had to manually discover the different modes and had no idea about their relative importance, `dynesty` properly sampled the full pdf."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python pymc3",
   "language": "python",
   "name": "pymc3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
