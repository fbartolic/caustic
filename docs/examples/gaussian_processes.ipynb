{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "%run notebook_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`caustic` enables the use of [Gaussian Processes](https://distill.pub/2019/visual-exploration-gaussian-processes/) for modeling correlated noise. The basic idea behind GPs is to extend the covariance matrix of the multivariate gaussian likelihood we've used in the previous tutorial and model the covariance matrix terms by means of a kernel function\n",
    "which depends on the difference between any two time points at which we measured the flux. That is\n",
    "   \n",
    "$$\\kappa(t)=\\kappa \\,(|t-t'|)$$\n",
    "\n",
    "Such a kernel is said to be stationary because it defines a stationary gaussian process. The covariance matrix terms are then\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\Sigma}_{i, j}=\\kappa\\left(|t_{i}- t_{j}|\\right)+\\sigma_{i}^{2} \\delta_{i, j}\n",
    "$$\n",
    "\n",
    "where $\\sigma_i$ are the \"error bars\" provided by photometry reduction pipelines. Because the covariance matrix is no longer diagonal, and the likelihood function involves computing its inverse and the determinant, naive implementations are extremely costly because the computation of a matrix inverse scales like $\\mathcal{O}(N^3)$. Fortunately, the recent\n",
    "package [celerité](https://celerite.readthedocs.io/en/stable/) enables computation of the gaussian process likelihood in linear time. It does this by restricting the application of GPs to one-dimensional data and a special class of kernel functions which result enable efficient computation of the inverse and the determinant of the covariance matrix. The restricted class of kernels is still sufficient for use in microlensing data. \n",
    "\n",
    "For more info on celerité, see the package documentation and the associated [paper](https://ui.adsabs.harvard.edu/abs/2017AJ....154..220F/abstract)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import pymc3 as pm\n",
    "import theano.tensor as T\n",
    "\n",
    "import caustic as ca\n",
    "import exoplanet as xo\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "event_ogle = ca.data.OGLEData(\"../../data/OGLE-2017-BLG-0660\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "event_ogle.plot_standardized_data(ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first fit a model with a diagonal covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a SingleLensModel object\n",
    "model = ca.models.SingleLensModel(event_ogle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bands = len(event_ogle.light_curves)\n",
    "BoundedNormal = pm.Bound(pm.Normal, lower=0.0)\n",
    "BoundedNormal_1 = pm.Bound(pm.Normal, lower=1.0)\n",
    "\n",
    "with model:\n",
    "    # Flux parameters\n",
    "    Delta_F = BoundedNormal(\n",
    "        \"Delta_F\",\n",
    "        mu=T.zeros(n_bands),\n",
    "        sd=50.0 * T.ones(n_bands),\n",
    "        testval=5.0 * T.ones(n_bands),\n",
    "        shape=(n_bands),\n",
    "    )\n",
    "\n",
    "    F_base = pm.Normal(\n",
    "        \"F_base\",\n",
    "        mu=T.zeros(n_bands),\n",
    "        sd=0.6 * T.ones(n_bands),\n",
    "        testval=T.zeros(n_bands),\n",
    "        shape=(n_bands),\n",
    "    )\n",
    "\n",
    "    # Other parameters\n",
    "    t_0 = pm.Uniform(\n",
    "        \"t_0\", model.t_min, model.t_max, testval=ca.estimate_t0(event_ogle)\n",
    "    )\n",
    "\n",
    "    u_0 = BoundedNormal(\"u_0\", mu=0.0, sd=1.5, testval=0.1)\n",
    "\n",
    "    teff = BoundedNormal(\"t_eff\", mu=0.0, sd=365.0, testval=20.0)\n",
    "\n",
    "    # Deterministic transformations\n",
    "    t_E = pm.Deterministic(\"t_E\", teff / u_0)\n",
    "    m_source, g = ca.compute_source_mag_and_blend_fraction(\n",
    "        event_ogle, model, Delta_F, F_base, u_0\n",
    "    )\n",
    "    pm.Deterministic(\"m_source\", m_source)\n",
    "    pm.Deterministic(\"g\", g)\n",
    "\n",
    "    # Compute the trajectory of the lens\n",
    "    trajectory = ca.trajectory.Trajectory(event_ogle, t_0, u_0, t_E)\n",
    "    u = trajectory.compute_trajectory(model.t)\n",
    "\n",
    "    # Compute the magnification\n",
    "    mag = model.compute_magnification(u, u_0)\n",
    "\n",
    "    # Compute the mean model\n",
    "    mean = Delta_F * mag + F_base\n",
    "\n",
    "    # Let's allow for rescaling of the error bars by a constant factor plus an additive term\n",
    "    c_1 = BoundedNormal_1(\n",
    "        \"c_1\",\n",
    "        mu=T.ones(n_bands),\n",
    "        sd=2.0 * T.ones(n_bands),\n",
    "        testval=1.5 * T.ones(n_bands),\n",
    "        shape=(n_bands),\n",
    "    )\n",
    "\n",
    "    c_2 = BoundedNormal(\n",
    "        \"c_2\",\n",
    "        mu=T.ones(n_bands),\n",
    "        sd=1.0 * T.ones(n_bands),\n",
    "        testval=0.1 * T.ones(n_bands),\n",
    "        shape=(n_bands),\n",
    "    )\n",
    "\n",
    "    # Diagonal terms of the covariance matrix\n",
    "    var_F = (c_1 * model.sig_F) ** 2 + c_2 ** 2\n",
    "\n",
    "    # Compute the Gaussian log_likelihood, add it as a potential term to the model\n",
    "    ll = model.compute_log_likelihood(model.F - mean, var_F)\n",
    "    pm.Potential(\"log_likelihood\", ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    # Print initial logps\n",
    "    initial_logps = [RV.logp(model.test_point) for RV in model.basic_RVs]\n",
    "    print(\"Initial values of log priors:\", initial_logps)\n",
    "\n",
    "    # Run sampling\n",
    "    trace = pm.sample(\n",
    "        tune=500, draws=1000, cores=4, step=xo.get_dense_nuts_step(target_accept=0.9)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.summary(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.plot_posterior(trace, figsize=(12, 12));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    # Create dense grid\n",
    "    t_dense = np.tile(np.linspace(model.t_min, model.t_max, 2000), (n_bands, 1))\n",
    "    t_dense_tensor = T.as_tensor_variable(t_dense)\n",
    "\n",
    "    # Compute the trajectory of the lens\n",
    "    u_dense = trajectory.compute_trajectory(t_dense_tensor)\n",
    "\n",
    "    # Compute the magnification\n",
    "    mag_dense = model.compute_magnification(u_dense, u_0)\n",
    "\n",
    "    # Compute the mean model\n",
    "    mean_dense = Delta_F * mag_dense + F_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot model\n",
    "fig, ax = plt.subplots(\n",
    "    2, 1, gridspec_kw={\"height_ratios\": [3, 1]}, figsize=(10, 5), sharex=True\n",
    ")\n",
    "\n",
    "ca.plot_model_and_residuals(\n",
    "    ax, event_ogle, model, trace, t_dense_tensor, mean_dense, n_samples=50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see clear correlations in the residuals which aren't accounted for by the model. To expand the model we include a GP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a SingleLensModel object\n",
    "model_gp = ca.models.SingleLensModel(event_ogle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bands = len(event_ogle.light_curves)\n",
    "BoundedNormal = pm.Bound(pm.Normal, lower=0.0)\n",
    "BoundedNormal_1 = pm.Bound(pm.Normal, lower=1.0)\n",
    "\n",
    "with model_gp:\n",
    "    # Flux parameters\n",
    "    Delta_F = BoundedNormal(\n",
    "        \"Delta_F\",\n",
    "        mu=T.zeros(n_bands),\n",
    "        sd=50.0 * T.ones(n_bands),\n",
    "        testval=5.0 * T.ones(n_bands),\n",
    "        shape=(n_bands, 1),\n",
    "    )\n",
    "\n",
    "    F_base = pm.Normal(\n",
    "        \"F_base\",\n",
    "        mu=T.zeros(n_bands),\n",
    "        sd=0.6 * T.ones(n_bands),\n",
    "        testval=T.zeros(n_bands),\n",
    "        shape=(n_bands),\n",
    "    )\n",
    "\n",
    "    # Other parameters\n",
    "    t_0 = pm.Uniform(\n",
    "        \"t_0\", model_gp.t_min, model_gp.t_max, testval=ca.estimate_t0(event_ogle)\n",
    "    )\n",
    "\n",
    "    u_0 = BoundedNormal(\"u_0\", mu=0.0, sd=1.5, testval=0.1)\n",
    "\n",
    "    teff = BoundedNormal(\"t_eff\", mu=0.0, sd=365.0, testval=20.0)\n",
    "\n",
    "    # Deterministic transformations\n",
    "    t_E = pm.Deterministic(\"t_E\", teff / u_0)\n",
    "    m_source, g = ca.compute_source_mag_and_blend_fraction(\n",
    "        event_ogle, model_gp, Delta_F, F_base, u_0\n",
    "    )\n",
    "    pm.Deterministic(\"m_source\", m_source)\n",
    "    pm.Deterministic(\"g\", g)\n",
    "\n",
    "    # Compute the trajectory of the lens\n",
    "    trajectory = ca.trajectory.Trajectory(event_ogle, t_0, u_0, t_E)\n",
    "    u = trajectory.compute_trajectory(model_gp.t)\n",
    "\n",
    "    # Compute the magnification\n",
    "    mag = model_gp.compute_magnification(u, u_0)\n",
    "\n",
    "    # Compute the mean model\n",
    "    mean = Delta_F * mag + F_base\n",
    "\n",
    "    # Let's allow for rescaling of the error bars by a constant factor plus an additive term\n",
    "    c_1 = BoundedNormal_1(\n",
    "        \"c_1\",\n",
    "        mu=T.ones(n_bands),\n",
    "        sd=2.0 * T.ones(n_bands),\n",
    "        testval=1.5 * T.ones(n_bands),\n",
    "        shape=(n_bands),\n",
    "    )\n",
    "\n",
    "    c_2 = BoundedNormal(\n",
    "        \"c_2\",\n",
    "        mu=T.ones(n_bands),\n",
    "        sd=1.0 * T.ones(n_bands),\n",
    "        testval=0.1 * T.ones(n_bands),\n",
    "        shape=(n_bands),\n",
    "    )\n",
    "\n",
    "    # Diagonal terms of the covariance matrix\n",
    "    var_F = (c_1 * model_gp.sig_F) ** 2 + c_2 ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the version of `celerite` implemented in the `exoplanet` code because it naturaly interfaces with `PyMC3` and provides gradient of the log likelihood with respect to the GP hyperparameters which is what we need for HMC to work. For more details check out the [exoplanet docs](https://exoplanet.dfm.io/en/latest/tutorials/gp/). We'll use the `exoplanet.gp.terms.Matern32` because it is a sensible default. This kernel is defined by two parameters, the characteristic lengthscale $\\rho$ (in our case $\\rho$ has dimensions of time since we're dealing with time series data), and $\\sigma$ which controls the spread in the dependent variable (the flux). We have to be caref about choosing priors for the lengthscale parameter because GPs are somewhat prone to overfitting. Following the suggestions in this [Stan case study](https://betanalpha.github.io/assets/case_studies/gp_part1/part1.html), we opt to use an Inverse Gamma prior for $\\rho$ which assigns 1\\% probability to timescales less than the median separation between consecutive data points and 1\\% probability to lengthscales larger than the entire duration of the time series. This is a sensible prior because it prevents the model from converging to timescales for which there is justification in the data. To compute the parameters of the Inverse Gamma distribution which satisfy the above requirements, we use the function `ca.compute_invgama_params`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model_gp:\n",
    "    # Initialize the GP parameters for the Matern32 kernel\n",
    "    sigma_gp = BoundedNormal(\n",
    "        \"sigma_gp\",\n",
    "        mu=T.zeros(n_bands),\n",
    "        sd=3.0 * T.ones(n_bands),\n",
    "        testval=0.5 * T.ones(n_bands),\n",
    "        shape=(n_bands, 1),\n",
    "    )\n",
    "\n",
    "    rho_gp = BoundedNormal(\n",
    "        \"rho_gp\",\n",
    "        mu=T.zeros(n_bands),\n",
    "        sd=100.0 * T.ones(n_bands),\n",
    "        testval=2.0 * T.ones(n_bands),\n",
    "        shape=(n_bands),\n",
    "    )\n",
    "\n",
    "    # List for storing xo.gp.GP objects\n",
    "    gp_list = []\n",
    "\n",
    "    for n in range(n_bands):\n",
    "        kernel = xo.gp.terms.Matern32Term(sigma=sigma_gp[n], rho=rho_gp[n])\n",
    "        gp_list.append(xo.gp.GP(kernel, model_gp.t[n], var_F[n], J=2))\n",
    "\n",
    "    # Compute the Gaussian log_likelihood, add it as a potential term to the model\n",
    "    ll = model_gp.compute_log_likelihood(model_gp.F - mean, var_F, gp_list)\n",
    "    pm.Potential(\"log_likelihood\", ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model_gp:\n",
    "    # Print initial logps\n",
    "    initial_logps = [RV.logp(model_gp.test_point) for RV in model_gp.basic_RVs]\n",
    "    print(\"Initial values of log priors:\", initial_logps)\n",
    "\n",
    "    # Run sampling\n",
    "    trace_gp = pm.sample(\n",
    "        tune=500, draws=2000, cores=4, step=xo.get_dense_nuts_step(target_accept=0.9)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.summary(trace_gp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.traceplot(trace_gp);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.plot_posterior(trace_gp, figsize=(12, 12));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.pairplot(\n",
    "    trace_gp,\n",
    "    figsize=(12, 10),\n",
    "    var_names=[\"Delta_F\", \"F_base\", \"c_1\", \"t_0\", \"u_0\", \"t_eff\", \"rho_gp\", \"sigma_gp\"],\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model_gp:\n",
    "    # Create dense grid\n",
    "    t_dense = np.tile(np.linspace(model_gp.t_min, model_gp.t_max, 1000), (n_bands, 1))\n",
    "    t_dense_tensor = T.as_tensor_variable(t_dense)\n",
    "\n",
    "    # Compute the trajectory of the lens\n",
    "    u_dense = trajectory.compute_trajectory(t_dense_tensor)\n",
    "\n",
    "    # Compute the magnification\n",
    "    mag_dense = model_gp.compute_magnification(u_dense, u_0)\n",
    "\n",
    "    # Compute the mean model\n",
    "    mean_dense = Delta_F * mag_dense + F_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will take some time..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot model\n",
    "fig, ax = plt.subplots(\n",
    "    2, 1, gridspec_kw={\"height_ratios\": [3, 1]}, figsize=(10, 8), sharex=True\n",
    ")\n",
    "\n",
    "ca.plot_model_and_residuals(\n",
    "    ax,\n",
    "    event_ogle,\n",
    "    model_gp,\n",
    "    trace_gp,\n",
    "    t_dense_tensor,\n",
    "    mean_dense,\n",
    "    n_samples=50,\n",
    "    gp_list=gp_list,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.plots.densityplot(\n",
    "    [trace, trace_gp],\n",
    "    point_estimate=\"median\",\n",
    "    figsize=(12, 12),\n",
    "    data_labels=[\"white noise model\", \"gp model\"],\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the GP easily converged and there are no clear patterns in the residuals of the model. We also see substantial differences in the posteriors for the physical parameters, the point estimates are different and the variance of parameters is generally larger for the GP model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pymc3))",
   "language": "python",
   "name": "pymc3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
