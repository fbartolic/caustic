{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import scipy\n",
    "import corner\n",
    "import pymc3 as pm\n",
    "import seaborn as sns\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.ifelse import ifelse\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../theano_ops\")\n",
    "sys.path.append(\"../codebase\")\n",
    "from data_preprocessing_ogle import process_data\n",
    "from plotting_utils import plot_data\n",
    "from theano_ops.celerite.factor import FactorOp\n",
    "from theano_ops.celerite.solve import SolveOp\n",
    "from theano_ops.celerite import terms\n",
    "from theano_ops.celerite.celerite import log_likelihood\n",
    "\n",
    "from scipy.special import gamma\n",
    "from scipy.stats import invgamma\n",
    "from scipy.optimize import fsolve\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DFM's pymc3 hack for estimating off-diagonal mass-matrix terms in NUTS during\n",
    "# burn-in period\n",
    "from pymc3.step_methods.hmc.quadpotential import QuadPotentialFull\n",
    "def get_step_for_trace(trace=None, model=None,\n",
    "                       regular_window=5, regular_variance=1e-3,\n",
    "                       **kwargs):\n",
    "    model = pm.modelcontext(model)\n",
    "    \n",
    "    # If not given, use the trivial metric\n",
    "    if trace is None:\n",
    "        potential = QuadPotentialFull(np.eye(model.ndim))\n",
    "        return pm.NUTS(potential=potential, **kwargs)\n",
    "        \n",
    "    # Loop over samples and convert to the relevant parameter space;\n",
    "    # I'm sure that there's an easier way to do this, but I don't know\n",
    "    # how to make something work in general...\n",
    "    samples = np.empty((len(trace) * trace.nchains, model.ndim))\n",
    "    i = 0\n",
    "    for chain in trace._straces.values():\n",
    "        for p in chain:\n",
    "            samples[i] = model.bijection.map(p)\n",
    "            i += 1\n",
    "    \n",
    "    # Compute the sample covariance\n",
    "    cov = np.cov(samples, rowvar=0)\n",
    "    \n",
    "    # Stan uses a regularized estimator for the covariance matrix to\n",
    "    # be less sensitive to numerical issues for large parameter spaces.\n",
    "    # In the test case for this blog post, this isn't necessary and it\n",
    "    # actually makes the performance worse so I'll disable it, but I\n",
    "    # wanted to include the implementation here for completeness\n",
    "    N = len(samples)\n",
    "    cov = cov * N / (N + regular_window)\n",
    "    cov[np.diag_indices_from(cov)] += \\\n",
    "        regular_variance * regular_window / (N + regular_window)\n",
    "    \n",
    "    # Use the sample covariance as the inverse metric\n",
    "    potential = QuadPotentialFull(cov)\n",
    "    return pm.NUTS(potential=potential, **kwargs)\n",
    "\n",
    "def solve_for_invgamma_params(params, x_min, x_max):\n",
    "    \"\"\"Returns parameters of an inverse gamma distribution p(x) such that \n",
    "    0.1% of total prob. mass is assigned to values of x < x_min and \n",
    "    1% of total prob. masss  to values greater than x_max.\"\"\"\n",
    "    def inverse_gamma_cdf(x, alpha, beta):\n",
    "        return invgamma.cdf(x, alpha, scale=beta)\n",
    "\n",
    "    lower_mass = 0.001\n",
    "    upper_mass = 0.99\n",
    "\n",
    "    # Trial parameters\n",
    "    alpha, beta = params\n",
    "\n",
    "    # Equation for the roots defining params which satisfy the constraint\n",
    "    return (inverse_gamma_cdf(2*x_min, alpha, beta) - \\\n",
    "    lower_mass, inverse_gamma_cdf(x_max, alpha, beta) - upper_mass)\n",
    "\n",
    "\n",
    "def fit_pymc3_model(t, F, sigF):\n",
    "    model = pm.Model()\n",
    "\n",
    "    # SPECIFICATION OF PRIORS\n",
    "    # Compute parameters for the prior on GP hyperparameters\n",
    "    invgamma_a, invgamma_b =  fsolve(solve_for_invgamma_params, (0.1, 0.1), \n",
    "        (np.median(np.diff(t)), t[-1] - t[0]))\n",
    "\n",
    "    strt = time.time()\n",
    "    with model:    \n",
    "        # Priors for GP hyperparameters\n",
    "        def ln_rho_prior(ln_rho):\n",
    "            lnpdf_lninvgamma = lambda  x, a, b: np.log(x) + a*np.log(b) -\\\n",
    "                 (a + 1)*np.log(x) - b/x - np.log(gamma(a)) \n",
    "\n",
    "            res = lnpdf_lninvgamma(np.exp(ln_rho), invgamma_a, invgamma_b)\n",
    "            return T.cast(res, 'float64')\n",
    "\n",
    "        ln_rho = pm.DensityDist('ln_rho', ln_rho_prior, testval = 0.6)\n",
    "\n",
    "        def ln_sigma_prior(ln_sigma):\n",
    "            sigma = np.exp(ln_sigma)\n",
    "            res = np.log(sigma) - sigma**2/3.**2\n",
    "            return T.cast(res, 'float64')\n",
    "\n",
    "        ln_sigma = pm.DensityDist('ln_sigma', ln_sigma_prior, testval=2.)\n",
    "\n",
    "#         u_K = pm.Uniform('u_K', -1., 1.)\n",
    "\n",
    "#         K = ifelse(u_K < 0., T.cast(1., 'float64'), 1. - T.log(1. - u_K))\n",
    "\n",
    "        # CALCULATE LIKELIHOOD\n",
    "        def custom_log_likelihood(t, F, sigF):\n",
    "            mean_function = 0.\n",
    "            kernel = terms.Matern32Term(sigma=ln_sigma, rho=ln_rho)\n",
    "            loglike = log_likelihood(kernel, mean_function,\n",
    "                sigF, t, F)\n",
    "\n",
    "            return loglike \n",
    "\n",
    "        logl = pm.DensityDist('logl', custom_log_likelihood, \n",
    "            observed={'t': t, 'F':F, 'sigF': sigF})\n",
    "\n",
    "        for RV in model.basic_RVs:\n",
    "            print(RV.name, RV.logp(model.test_point))\n",
    "\n",
    "        # Fit model with NUTS\n",
    "        #trace = pm.sample(2000, tune=2000, nuts_kwargs=dict(target_accept=.95),\n",
    "        #    start=start)\n",
    "\n",
    "        # DFM's optimized sampling procedure\n",
    "        start = None\n",
    "        burnin_trace = None\n",
    "        for steps in n_window:\n",
    "            step = get_step_for_trace(burnin_trace, regular_window=0)\n",
    "            burnin_trace = pm.sample(\n",
    "                start=start, tune=steps, draws=2, step=step,\n",
    "                compute_convergence_checks=False, discard_tuned_samples=False)\n",
    "            start = [t[-1] for t in burnin_trace._straces.values()]\n",
    "\n",
    "        step = get_step_for_trace(burnin_trace, regular_window=0)\n",
    "        dense_trace = pm.sample(draws=5000, tune=n_burn, step=step, start=start)\n",
    "        factor = 5000 / (5000 + np.sum(n_window+2) + n_burn)\n",
    "        dense_time = factor * (time.time() - strt)\n",
    "\n",
    "    return dense_trace, dense_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded events: ['blg-0001']\n",
      "ln_rho -1.3731855828410553\n",
      "ln_sigma -4.066461114793804\n",
      "logl -439.0106413406179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Only 2 samples in chain.\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [ln_sigma, ln_rho]\n",
      "Sampling 4 chains:  27%|██▋       | 29/108 [00:19<00:00, 154.33draws/s]"
     ]
    }
   ],
   "source": [
    "events = [] # event names\n",
    "lightcurves = [] # data for each event\n",
    " \n",
    "i = 0\n",
    "n_events = 1\n",
    "for entry in os.scandir('/home/star/fb90/data/OGLE_ews/2017/'):\n",
    "    if entry.is_dir() and (i < n_events):\n",
    "        events.append(entry.name)\n",
    "        photometry = np.genfromtxt(entry.path + '/phot.dat', usecols=(0,1,2))\n",
    "        lightcurves.append(photometry)\n",
    "        i = i + 1\n",
    "        \n",
    "print(\"Loaded events:\", events)\n",
    "\n",
    "# Define a tuning schedule for HMC\n",
    "n_start = 25\n",
    "n_burn = 500\n",
    "n_tune = 5000\n",
    "n_window = n_start * 2 ** np.arange(np.floor(np.log2((n_tune - n_burn) / n_start)))\n",
    "n_window = np.append(n_window, n_tune - n_burn - np.sum(n_window))\n",
    "n_window = n_window.astype(int)\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "for event_index, lightcurve in enumerate(lightcurves):\n",
    "    # Pre process the data\n",
    "    t, F, sigF = process_data(lightcurve[:, 0], lightcurve[:, 1], \n",
    "        lightcurve[:, 2], standardize=True)\n",
    "    \n",
    "    t = t[:500]\n",
    "    F = F[:500]\n",
    "    sigF = sigF[:500]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(20, 6))\n",
    "    fig2, ax2 = plt.subplots(figsize=(20, 6))\n",
    "\n",
    "    plot_data(ax, t, F, sigF)\n",
    "\n",
    "    # Fit pymc3 model\n",
    "    dense_trace, dense_time = fit_pymc3_model(t, F, sigF)\n",
    "    stats = pm.summary(dense_trace)\n",
    "    dense_time_per_eff = dense_time / stats.n_eff.min()\n",
    "    print(\"time per effective sample: {0:.5f} ms\".format(dense_time_per_eff * 1000))\n",
    "\n",
    "    fig, ax = plt.subplots(5, 2 ,figsize=(10,10))\n",
    "    _ = pm.traceplot(dense_trace, ax=ax2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
